1.3
1. Что будет, если инициализироваться на стеке основного потока, и сделать pthread_exit, пока порожденные еще живы?
   А если при этом в порожденных запускать новые потока?
2. А если выделить возвращаемую строку на стеке порожденного потока, дождаться завершения и запустить еще один,
   получится ли прочитать значение?
3. Если решили, что выделять на куче лучше - подумайте, чем нехорошим это чревато, в особенности при командной разработке.
   Попробуйте реализовать какой-нибудь из пришедших в голову способов смягчить влияние этого нехорошего в коде.

   При экспериментировании можно выделять на стеке сначала большую структуру, а в другой поточной функции поменьше,
   и искать ошметки большой на стеке потока, запущенного после завершения первого. Либо, смотреть maps и адреса стеков

1.4
1. Что делать, если нет возможности инструментировать (дополнить) код поточной функции, а она не останавливается
   "стандартным" pthread_cancel? Как должен работать механизм, который обеспечит остановку в таком случае?
   Подсказка: pthread_cancel тут тоже поможет, но с некоторыми приседаниями.
   Как должен работать = можно посмотреть в исходниках, а можно подумать с точки зрения разработчика ОС, не обязательно линукса.

1.5
1. Cоздайте четвертый поток без обработки сигналов. Попробуйте использовать для отправки сигналов kill из командной строки.
   Обьясните результат.
2. Создайте четвертый поток, аналогичный второму или третьему, но с другим сообщением об обработке сигнала. 
   Попробуйте отправлять различные сигналы различными способами. Обьясните результат. Всегда ли будет такое поведение?

   Для п.2 ответы есть в исходниках, в том числе

1.6
1. Хочу mythread_join. Можно сколь угодно примитивный, главное, чтобы был.
2. Хочу, чтобы по mythread_create при ошибке какой-то внутрянки было понятно, что произошло. Можно даже секцию мана для нее написать,
   как понимать какие значения errno. Если нужно, подкорректируйте их в обработке ошибок внутри функции.
3. mythread_cancel писать не нужно, но как бы вы его сделали - могу спросить.

1.7
1. Если реализовали без вытеснения потока, то в примере, которым доказываете работоспособность, каждая потоковая функция должна
   yield'ить (отдавать поток управления) хотя бы несколько раз. Т.е. нельзя делать последовательное выполнение 
   "1 поток начался, 1 поток закончился, 2 поток начался 2 поток закончился 3 поток начался" .. без возврата к 1 или 2 потоку.

2.1
1. Наверное, единственное, что не укладывается в ответы сходу - экспериментальное обоснование
   ответа на пункт про "поиграть длиной очереди". 
   Посмотрите на заполняемость очереди - как вариант, писать лог, когда не добавляем в полную или не читаем из пустой очереди.

2.2
0. Про мутекс и бинарный семафор - зачем одно, если есть другое?
   если нашли ответ, не поленитесь его проверить - так ли это? 
   если получилось не так, то что надо исправить в коде?
1. Загружает ли реализация на мутексах процессор почти на уровне спинлока?
   попробуйте исправить, вставив в нужном месте usleep.
   поэкспериментируйте со значением usleep, чтобы производительность была на уровне остальных вариантов
2. Интересны итоговые цифры производительности и времени в ядре/юзерспейсе, и обьяснение, почему так

2.3
1. Условие сформулировано как попало, почти любые прочтения допускаются, лишь бы была конкуренция.
   Лично мне самым интересным кажется, где процессы свапперы не бегают линейно и рандомно решают, менять ли,
   а где они рандомно выбирают само место замены и начала блокировок. Так они будут с большей вероятностью дедлочиться (почему?)
2. Следите за консистентностью - если в вашей функции чтения или изменения примитивы отпускаются и захватываются несколько раз,
   то либо добавляйте логи и показывайте, что там в момент между unlock и lock кто-то вмешивается, или вообще получается дедлок по логам отследить,
   либо исправляйте и доказывайте, что все операции не зависят от вмешательства других процессов, либо его не допускают.
3. Сравнить мутексы и rwlock по итоговым параметрам, обьяснить, почему так.

2.4
Как правило, реализовать несложно.
1. Мне хочется, чтобы не оставалось ощущения, что использование футекса, атомарного CAS и чего-нибудь еще необычного (atomic_int?)
   воспринималось как применение магии, про которую сказали что она сработает - поэтому буду спрашивать, что именно оно делает.
   Для этого может быть хорошей идеей:
   * если ассемблерная вставка - то покажите ее,
   * если футекс - расскажите, что там внутри.
   Для этого может быть полезным заглянуть в линуксовую реализацию.
2. Сравнить грубую реализацию мутекса с pthread_mutex; Обьяснить.
* какой-нибудь общий тестовый сценарий на разных мутексах
* для "обьяснить" опять же можно смотреть, что там в pthread_mutex.

минипроектные лабы

0. Проверяйте, что оно работает
1. Проверяйте, что оно работает где-то, кроме тривиального случая (1 коннект/копирование одного файла.)
Это может быть скриптик на кучу коннектов, попытки кэшировать видос, etc
2. Нарисуйте перед сдачей схему архитектуры лабы. Какие модули за что отвечают и что делают, etc.
Неплохим вариантом будет куда-то туда добавить инфу о использованных примитивах синхронизации, кто когда их захватывает.
Это все может ускорить сдачу.